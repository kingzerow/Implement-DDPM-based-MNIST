{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:36:55.762632900Z",
     "start_time": "2024-10-31T01:36:55.704260Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from typing import Union\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:36:55.763644600Z",
     "start_time": "2024-10-31T01:36:55.707297800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dir= 'data/mnist/train/mnist_train.csv'\n",
    "test_dir= 'data/mnist/test/mnist_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:36:55.838240500Z",
     "start_time": "2024-10-31T01:36:55.718459500Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_mnist(file_name):\n",
    "    mnist= []\n",
    "    with open(file_name, encoding= 'utf-8') as f:\n",
    "        mnist= f.readlines()\n",
    "    rows= len(mnist)\n",
    "    mnist= np.array([int(item) for sting in mnist for item in sting.split(',')]).reshape(rows, -1)\n",
    "    return torch.from_numpy(mnist[:, 1:]).float()/ 255.0, torch.from_numpy(mnist[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.670716300Z",
     "start_time": "2024-10-31T01:36:55.726578100Z"
    }
   },
   "outputs": [],
   "source": [
    "seed, batch_size= 1, 512\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "pics, labels= read_mnist(train_dir)\n",
    "random_num= torch.rand(len(pics))\n",
    "train_flag, valid_flag, test_flag= random_num < torch.tensor(0.8), (random_num >= torch.tensor(0.8))* (random_num < torch.tensor(0.9)), random_num >= torch.tensor(0.9)\n",
    "train_pics, valid_pics, test_pics= pics[train_flag], pics[valid_flag], pics[test_flag]\n",
    "train_labels, valid_labels, test_labels= labels[train_flag], labels[valid_flag], labels[test_flag]\n",
    "train_set, valid_set, test_set= torch.utils.data.TensorDataset(train_pics, train_labels), torch.utils.data.TensorDataset(valid_pics, valid_labels), torch.utils.data.TensorDataset(test_pics, test_labels)\n",
    "train_loader, valid_loader, test_loader= torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle= True), torch.utils.data.DataLoader(valid_set, batch_size= batch_size, shuffle= True), torch.utils.data.DataLoader(test_set, batch_size= batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.679806600Z",
     "start_time": "2024-10-31T01:37:05.672735900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tools:\n",
    "    def gather(self, consts: torch.Tensor, t: torch.Tensor):\n",
    "        c = consts.gather(-1, t)\n",
    "        return c.reshape(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.689944400Z",
     "start_time": "2024-10-31T01:37:05.679806600Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenoiseDiffusion():\n",
    "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.eps_model= eps_model\n",
    "        self.beta= torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
    "        self.alpha= 1.0- self.beta\n",
    "        # compute cumulative product\n",
    "        self.alpha_bar= torch.cumprod(self.alpha, dim= 0)\n",
    "        self.n_steps= n_steps\n",
    "        self.sigma= self.beta\n",
    "        self.tools= Tools()\n",
    "\n",
    "    # forward- diffusion\n",
    "    def q_xt_x0(self, x0: torch.tensor, t:torch.Tensor):\n",
    "        # compute mean and var of xt according to x0\n",
    "        # xt= sqrt(at)*x0+ sqrt(1-at)*eps\n",
    "        maen= self.tools.gather(self.alpha_bar, t)** 0.5* x0\n",
    "        # (batch_size, 1, 1, 1)\n",
    "        var= 1- self.tools.gather(self.alpha_bar, t)\n",
    "        return maen, var\n",
    "\n",
    "    # forward- diffusion\n",
    "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor]= None):\n",
    "        # compute xt according mean and var of xt\n",
    "        if eps is None:\n",
    "            eps= torch.randn_like(x0)\n",
    "        maen, var= self.q_xt_x0(x0, t)\n",
    "        return maen+ (var** 0.5)* eps\n",
    "    \n",
    "    # sampling\n",
    "    def p_sample(self, xt: torch.tensor, t: torch.Tensor):\n",
    "        # compute xt-1 according xt\n",
    "        eps_hat= self.eps_model(xt, t)\n",
    "        alpha_bar= self.tools.gather(self.alpha_bar, t)\n",
    "        alpha= self.tools.gather(self.alpha, t)\n",
    "        eps_coef= (1- alpha)/ (1- alpha_bar)** 0.5\n",
    "        maen= 1/ (alpha** 0.5)* (xt- eps_coef* eps_hat)\n",
    "        var= self.tools.gather(self.sigma, t)\n",
    "        eps= torch.randn(xt.shape, device= xt.device)\n",
    "        return maen+ (var** 0.5)* eps\n",
    "    \n",
    "    # loss\n",
    "    # x0, (batch_size, C, H, W);\n",
    "    def loss(self, x0: torch.tensor, noise: Optional[torch.Tensor]= None):\n",
    "        # distance between loss\n",
    "        batch_size= x0.shape[0]\n",
    "        # (batch_size, )\n",
    "        t= torch.randint(0, self.n_steps, (batch_size, ), device= x0.device, dtype= torch.long)\n",
    "        if noise is None:noise= torch.randn_like(x0)\n",
    "        xt= self.q_sample(x0, t, eps= noise)\n",
    "        eps_hat= self.eps_model(xt, t)\n",
    "        return F.mse_loss(noise, eps_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.690955Z",
     "start_time": "2024-10-31T01:37:05.684893300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x* torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.706140200Z",
     "start_time": "2024-10-31T01:37:05.692976600Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBolck(nn.Module):\n",
    "    # n_groups, hyper-parameter of group norm\n",
    "    # group norm, group normalize; first, split channels into different groups; then, normalize feature in every group, as batch_normalization, it has some hyper-parameters.\n",
    "    # feat_map  \n",
    "    # cv1(feat_map)+ cv2(time_emb) -> feat_map' + cv(feat_map) -> output\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int= 16, dropout: float= 0.1):\n",
    "        super().__init__()\n",
    "        # in_channels// n_groups\n",
    "        self.norm1= nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1= Swish()\n",
    "        self.conv1= nn.Conv2d(in_channels, out_channels, kernel_size= (3, 3), padding= (1, 1))\n",
    "        self.norm2= nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2= Swish()\n",
    "        self.conv2= nn.Conv2d(out_channels, out_channels, kernel_size= (3, 3), padding= (1, 1))\n",
    "        if in_channels!= out_channels:\n",
    "            self.shortcut= nn.Conv2d(in_channels, out_channels, kernel_size= (1, 1))\n",
    "        else:\n",
    "            self.shortcut= nn.Identity()\n",
    "        self.time_emb= nn.Linear(time_channels, out_channels)\n",
    "        self.time_act= Swish()\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor, t:torch.Tensor):\n",
    "        # norm>> act>> conv\n",
    "        h= self.conv1(self.act1(self.norm1(x)))\n",
    "        # time embedding, (batch_size, out_channels, 1, 1)\n",
    "        # time embedding的不同特征与不同通道的特征图进行相加以实现空间与时间的融合\n",
    "        h+= self.time_emb(self.time_act(t))[:, :, None, None]\n",
    "        h= self.conv2(self.dropout(self.act2(self.norm2(h))))\n",
    "        return h+ self.shortcut(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.706140200Z",
     "start_time": "2024-10-31T01:37:05.700074900Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_channels: int, n_heads: int= 1, k_dims: int= None, n_groups: int= 16):\n",
    "        super().__init__()\n",
    "        if k_dims is None:\n",
    "            k_dims= n_channels\n",
    "        self.norm= nn.GroupNorm(n_groups, n_channels)\n",
    "        # as n_channels= 64, 64>> 8* 128* 3\n",
    "        self.projection= nn.Linear(n_channels, n_heads* k_dims* 3)\n",
    "        self.output= nn.Linear(n_heads* k_dims, n_channels)\n",
    "        self.scale= k_dims** -0.5\n",
    "        self.n_heads= n_heads\n",
    "        self.k_dims= k_dims\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor]= None):\n",
    "        _= t\n",
    "        batch_size, n_channels, height, weight= x.shape\n",
    "        # pull x straight, (batch_size, n_channels, H* W)>> (batch_size, H* W, n_channels)\n",
    "        x= x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # (batch_size, H* W, channels)>> (batch_size, H* W, head, 3* k_head_dim)\n",
    "        qkv= self.projection(x).view(batch_size, -1, self.n_heads, 3* self.k_dims)\n",
    "        # q, (batch_size, H* W, head, k_head_dim); k, (batch_size, H* W, head, k_head_dim); v, (,,).\n",
    "        q, k, v= torch.chunk(qkv, 3, dim= -1)\n",
    "        # (batch_size, H* W, head, dim), (batch_size, H* W, head, dim) -> (batch_size, H* W, H* W, head)\n",
    "        # This writing style is really good!\n",
    "        attn= torch.einsum('bihd,bjhd->bijh', q, k)* self.scale\n",
    "        attn= attn.softmax(dim= 2)\n",
    "        # (batch_size, H* W, H* W, head), (batch_size, H* W, head, dim)\n",
    "        res= torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # (batch_size, H* W, head* dim)\n",
    "        res= res.view(batch_size, -1, self.n_heads* self.k_dims)\n",
    "        # (batch_size, H* W, C)\n",
    "        res= self.output(res)\n",
    "        res+= x\n",
    "        res= res.permute(0, 2, 1).view(batch_size, n_channels, height, weight)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.708163800Z",
     "start_time": "2024-10-31T01:37:05.705130300Z"
    }
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    # Encoder\n",
    "    # DownBlock= ResidualBlock+ AttentionBlock\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res= ResidualBolck(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn= AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn= nn.Identity()\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x= self.res(x, t)\n",
    "        x= self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.719314300Z",
     "start_time": "2024-10-31T01:37:05.711199100Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_channels:int):\n",
    "        super().__init__()\n",
    "        self.n_channels= n_channels\n",
    "        # n_channels// 8 and concate\n",
    "        self.lin1= nn.Linear(self.n_channels// 4, self.n_channels)\n",
    "        self.act= Swish()\n",
    "        self.lin2= nn.Linear(self.n_channels, self.n_channels)\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        half_dim= self.n_channels// 8\n",
    "        emb= math.log(10000)/ (half_dim- 1)\n",
    "        emb= torch.exp(torch.arange(half_dim, device= t.device)* -emb)\n",
    "        emb= t[:, None]* emb[None, :]\n",
    "        emb= torch.cat((emb.sin(), emb.cos()), dim= 1)\n",
    "        # transform\n",
    "        emb= self.act(self.lin1(emb))\n",
    "        emb= self.lin2(emb)\n",
    "        return emb        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.720326300Z",
     "start_time": "2024-10-31T01:37:05.716279600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        # 反卷积，\n",
    "        self.conv= nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        _= t\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.730477600Z",
     "start_time": "2024-10-31T01:37:05.721340700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "    def forward(self, x:torch.tensor, t:torch.tensor):\n",
    "        _= t\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.756796500Z",
     "start_time": "2024-10-31T01:37:05.728417500Z"
    }
   },
   "outputs": [],
   "source": [
    "class MiddleBlock(nn.Module):\n",
    "    def __init__(self, n_channels:int, time_channels:int):\n",
    "        super().__init__()\n",
    "        self.res1= ResidualBolck(n_channels, n_channels, time_channels)\n",
    "        self.attn= AttentionBlock(n_channels)\n",
    "        self.res2= ResidualBolck(n_channels, n_channels, time_channels)\n",
    "    def forward(self, x:torch.tensor, t:torch.tensor):\n",
    "        x= self.res1(x, t)\n",
    "        x= self.attn(x)\n",
    "        x= self.res2(x, t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.758817500Z",
     "start_time": "2024-10-31T01:37:05.732506800Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # we concatenate the output of the same resolution\n",
    "        self.res= ResidualBolck(in_channels+ out_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn= AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn= nn.Identity()\n",
    "    def forward(self, x:torch.Tensor, t:torch.Tensor):\n",
    "        x= self.res(x, t)\n",
    "        x= self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:05.758817500Z",
     "start_time": "2024-10-31T01:37:05.738575500Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_channels: int= 1, n_channels: int= 16, ch_mults: Union[tuple[int, ...], List[int]]= [1, 2, 2, 2], is_attn: Union[Tuple[bool, ...], List[int]]= (False, False, False, False), n_blocks: int= 1):\n",
    "        super().__init__()\n",
    "        # \n",
    "        n_resolutions= len(ch_mults)\n",
    "        self.image_proj= nn.Conv2d(image_channels, n_channels, kernel_size= (3, 3), padding= (1, 1))\n",
    "        self.time_emb= TimeEmbedding(n_channels* 4)\n",
    "        # ---- Encoder ----\n",
    "        down= []\n",
    "        out_channels= in_channels= n_channels\n",
    "        # for each layers\n",
    "        for i in range(n_resolutions):\n",
    "            # 16* (1, 2, 2, 2)\n",
    "            out_channels= in_channels* ch_mults[i]\n",
    "            for _ in range(n_blocks):\n",
    "                # residual+ attn\n",
    "                down.append(DownBlock(in_channels, out_channels, n_channels* 4, is_attn[i]))\n",
    "                in_channels= out_channels\n",
    "            # downsample in each layer\n",
    "            if i< n_resolutions- 1:\n",
    "                # channels no change, (H, W)>> (H/2, W/ 2)\n",
    "                down.append(Downsample(in_channels))\n",
    "        self.down= nn.ModuleList(down)\n",
    "        # middle, residual+ attn+ residual\n",
    "        self.middle= MiddleBlock(out_channels, n_channels* 4)\n",
    "        # Decoder\n",
    "        up= []\n",
    "        in_channels= out_channels\n",
    "        # for each layer\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # \n",
    "            out_channels= in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                # residual+ attn\n",
    "                up.append(UpBlock(in_channels, out_channels, n_channels* 4, is_attn[i]))\n",
    "            out_channels= in_channels// ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_channels* 4, is_attn[i]))\n",
    "            in_channels= out_channels\n",
    "            if i> 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "        self.up= nn.ModuleList(up)\n",
    "        self.norm= nn.GroupNorm(8, n_channels)\n",
    "        self.act= Swish()\n",
    "        self.final= nn.Conv2d(in_channels, image_channels, kernel_size= (3, 3), padding= (1, 1))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        t= self.time_emb(t)\n",
    "        x= self.image_proj(x)\n",
    "        # Encoder\n",
    "        h= [x]\n",
    "        for m in self.down:\n",
    "            x= m(x, t)\n",
    "            h.append(x)\n",
    "        # Middle\n",
    "        x= self.middle(x, t)\n",
    "        # Decoder\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x= m(x, t)\n",
    "            else:\n",
    "                s= h.pop()\n",
    "                x= torch.cat((x, s), dim= 1)\n",
    "                x= m(x, t)\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:06.606350700Z",
     "start_time": "2024-10-31T01:37:05.747697700Z"
    }
   },
   "outputs": [],
   "source": [
    "lr, device, save_dir= 0.001, 'cpu', './u_net.pt'\n",
    "u_net= UNet(1, 16, [1, 2, 2], [False, False, False], n_blocks= 1).to(device)\n",
    "dm= DenoiseDiffusion(u_net, 1000, device= device)\n",
    "opt_dm= torch.optim.Adam(u_net.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:06.615460Z",
     "start_time": "2024-10-31T01:37:06.611402500Z"
    }
   },
   "outputs": [],
   "source": [
    "best_score, score, epochs, early_stop_time, early_stop_threshold= 1e10, 0, 0, 0, 40\n",
    "for epoch in range(epochs):\n",
    "    loss_record= []\n",
    "    for step, (pic, labels) in enumerate(train_loader):\n",
    "        pic= pic.view(-1, 1, 28, 28).to(device)\n",
    "        opt_dm.zero_grad()\n",
    "        loss= dm.loss(pic)\n",
    "        loss_record.append(loss.item())\n",
    "        loss.backward()\n",
    "        opt_dm.step()\n",
    "    print(f'training epoch: {epoch}, mean loss: {torch.tensor(loss_record).mean()}')\n",
    "    loss_record= []\n",
    "    with torch.no_grad():\n",
    "        for step, (pic, labels) in enumerate(valid_loader):\n",
    "            pic= pic.view(-1, 1, 28, 28).to(device)\n",
    "            loss= dm.loss(pic)\n",
    "            loss_record.append(loss.item()) \n",
    "    mean_loss= torch.tensor(loss_record).mean()\n",
    "    # early stopping\n",
    "    if mean_loss< best_score:\n",
    "        early_stop_time= 0\n",
    "        best_score= mean_loss\n",
    "        torch.save(u_net, f'{save_dir}')\n",
    "    else:\n",
    "        early_stop_time= early_stop_time + 1\n",
    "    if early_stop_time > early_stop_threshold:\n",
    "        break\n",
    "    # output\n",
    "    print(f'early_stop_time/early_stop_threshold: {early_stop_time}/{early_stop_threshold}, mean loss: {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:06.625576300Z",
     "start_time": "2024-10-31T01:37:06.616475200Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_sample(images, texts):\n",
    "    _, figs= plt.subplots(1, len(images), figsize= (12, 12))\n",
    "    for text, f, img in zip(texts, figs, images):\n",
    "        f.imshow(img.view(28, 28), cmap= 'gray')\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "        f.text(0.5, 0, text, ha= 'center', va= 'bottom', fontsize= 12, color= 'white', backgroundcolor= 'black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T01:37:16.820418100Z",
     "start_time": "2024-10-31T01:37:06.622543600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19119\\AppData\\Local\\Temp\\ipykernel_21712\\281168141.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  u_net= torch.load(f'{save_dir}', map_location= device)\n"
     ]
    }
   ],
   "source": [
    "xt, images, texts= torch.randn((1, 1, 28, 28), device= device), [], []\n",
    "u_net= torch.load(f'{save_dir}', map_location= device)\n",
    "dm= DenoiseDiffusion(u_net, 1000, device= device)\n",
    "for t in reversed(range(1000)):\n",
    "    xt_1= dm.p_sample(xt, torch.tensor([t]).to(device))\n",
    "    xt= xt_1\n",
    "    if (t+ 1)% 100== 1:\n",
    "        images.append(xt.view(1, 28, 28).to('cpu').detach())\n",
    "        texts.append(t+ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-31T01:37:16.822498800Z"
    }
   },
   "outputs": [],
   "source": [
    "images_= torch.stack(images, dim= 0)\n",
    "show_sample(images_, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
